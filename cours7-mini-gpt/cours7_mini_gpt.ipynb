{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ankueF7INp4f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from IPython.display import display, HTML\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paramètres du projet"
      ],
      "metadata": {
        "id": "e5VzkYCHQ8YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Taille du vocabulaire utilisé pour le modèle (nombre de mots uniques pris en compte)\n",
        "VOCAB_SIZE = 10000\n",
        "\n",
        "# Longueur maximale des séquences traitées par le modèle\n",
        "MAX_LEN = 80\n",
        "\n",
        "# Dimension de l'embedding des mots (représentation vectorielle des mots)\n",
        "EMBEDDING_DIM = 256\n",
        "\n",
        "# Dimension des clés pour l'attention multi-têtes\n",
        "KEY_DIM = 256\n",
        "\n",
        "# Nombre de \"têtes\" dans l'attention multi-têtes\n",
        "N_HEADS = 2\n",
        "\n",
        "# Dimension du réseau feed-forward dans le Transformer\n",
        "FEED_FORWARD_DIM = 256\n",
        "\n",
        "# Fraction des données utilisée pour la validation pendant l'entraînement\n",
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "# Graine aléatoire pour la reproductibilité\n",
        "SEED = 42\n",
        "\n",
        "# Si True, charge un modèle pré-entraîné au lieu de former un nouveau modèle\n",
        "LOAD_MODEL = False\n",
        "\n",
        "# Taille des lots utilisés pendant l'entraînement\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Nombre de cycles complets de passage sur l'ensemble de données pendant l'entraînement\n",
        "EPOCHS = 10\n"
      ],
      "metadata": {
        "id": "t4LqiA8tQ7CI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data"
      ],
      "metadata": {
        "id": "cio4LbWNRDYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les poèmes depuis un fichier txt\n",
        "with open(\"poeme_jour13.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    poems = [poem.lower() for poem in file.readlines()]\n",
        "\n",
        "\n",
        "\n",
        "# Visualisation des premiers mots pour vérification\n",
        "print(poems[:100])  # Affichage des 1000 premiers mots pour vérification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwFOI4JnRGGa",
        "outputId": "0dab7096-324a-4359-e96b-477be9c61014"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', '\\n', '\\n', 'il est temps que je me repose;\\n', 'je suis terrassé par le sort.\\n', \"ne me parlez pas d'autre chose\\n\", \"que des ténèbres où l'on dort!\\n\", '\\n', 'que veut-on que je recommence?\\n', 'je ne demande désormais\\n', 'à la création immense\\n', \"qu'un peu de silence et de paix!\\n\", '\\n', \"pourquoi m'appelez-vous encore?\\n\", \"j'ai fait ma tâche et mon devoir.\\n\", \"qui travaillait avant l'aurore,\\n\", \"peut s'en aller avant le soir.\\n\", '\\n', 'à vingt ans, deuil et solitude!\\n', 'mes yeux, baissés vers le gazon,\\n', 'perdirent la douce habitude\\n', 'de voir ma mère à la maison.\\n', '\\n', 'elle nous quitta pour la tombe;\\n', \"et vous savez bien qu'aujourd'hui\\n\", 'je cherche, en cette nuit qui tombe,\\n', \"un autre ange qui s'est enfui!\\n\", '\\n', 'vous savez que je désespère,\\n', 'que ma force en vain se défend,\\n', 'et que je souffre comme père,\\n', 'moi qui souffris tant comme enfant!\\n', '\\n', \"mon oeuvre n'est pas terminée,\\n\", 'dites-vous. comme adam banni,\\n', 'je regarde ma destinée,\\n', \"et je vois bien que j'ai fini.\\n\", '\\n', \"l'humble enfant que dieu m'a ravie\\n\", \"rien qu'en m'aimant savait m'aider;\\n\", \"c'était le bonheur de ma vie\\n\", 'de voir ses yeux me regarder.\\n', '\\n', \"si ce dieu n'a pas voulu clore\\n\", \"l'oeuvre qu'il me fit commencer,\\n\", \"s'il veut que je travaille encore,\\n\", \"il n'avait qu'à me la laisser!\\n\", '\\n', \"il n'avait qu'à me laisser vivre\\n\", 'avec ma fille à mes côtés,\\n', \"dans cette extase où je m'enivre\\n\", 'de mystérieuses clartés!\\n', '\\n', \"ces clartés, jour d'une autre sphère,\\n\", 'o dieu jaloux, tu nous les vends!\\n', \"pourquoi m'as-tu pris la lumière\\n\", \"que j'avais parmi les vivants?\\n\", '\\n', 'as-tu donc pensé, fatal maître,\\n', \"qu'à force de te contempler,\\n\", 'je ne voyais plus ce doux être,\\n', \"et qu'il pouvait bien s'en aller!\\n\", '\\n', \"t'es-tu dit que l'homme, vaine ombre,\\n\", 'hélas! perd son humanité\\n', 'a trop voir cette splendeur sombre\\n', \"qu'on appelle la vérité?\\n\", '\\n', \"qu'on peut le frapper sans qu'il souffre,\\n\", \"que son coeur est mort dans l'ennui,\\n\", \"et qu'à force de voir le gouffre,\\n\", \"il n'a plus qu'un abîme en lui?\\n\", '\\n', \"qu'il va, stoïque, où tu l'envoies,\\n\", 'et que désormais, endurci,\\n', \"n'ayant plus ici-bas de joies,\\n\", \"il n'a plus de douleurs aussi?\\n\", '\\n', \"as-tu pensé qu'une âme tendre\\n\", \"s'ouvre à toi pour se mieux fermer,\\n\", 'et que ceux qui veulent comprendre\\n', 'finissent par ne plus aimer?\\n', '\\n', 'o dieu! vraiment, as-tu pu croire\\n', 'que je préférais, sous les cieux,\\n', \"l'effrayant rayon de ta gloire\\n\", 'aux douces lueurs de ses yeux!\\n', '\\n', \"si j'avais su tes lois moroses,\\n\", \"et qu'au même esprit enchanté\\n\", 'tu ne donnes point ces deux choses,\\n', 'le bonheur et la vérité,\\n', '\\n', 'plutôt que de lever tes voiles,\\n', 'et de chercher, coeur triste et pur,\\n', 'a te voir au fond des étoiles,\\n', \"o dieu sombre d'un monde obscur,\\n\", '\\n', \"j'eusse aimé mieux, loin de ta face,\\n\", 'suivre, heureux, un étroit chemin,\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation_and_newlines(s):\n",
        "    # Ajouter de l'espace avant chaque signe de ponctuation\n",
        "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1\", s)\n",
        "\n",
        "    # Supprimer les caractères de ponctuation et les retours à la ligne\n",
        "    s = re.sub(f\"[{string.punctuation}\\n]\", \"\", s)\n",
        "\n",
        "    # Supprimer les espaces multiples\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "\n",
        "    return s.strip()\n",
        "\n",
        "\n",
        "\n",
        "text_data = [remove_punctuation_and_newlines(x) for x in poems]"
      ],
      "metadata": {
        "id": "FYihag0BRJWp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_data = text_data[25]\n",
        "example_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qHgZ_ftLRMTx",
        "outputId": "580dda83-1443-46ee-ebae-e4df9f1f4088"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'je cherche en cette nuit qui tombe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comptez les poèmes\n",
        "n_poems = len(text_data)\n",
        "print(f\"{n_poems}   lignes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuzVl-FaRP-N",
        "outputId": "3cefad8a-43ac-444b-9b4d-536cecf286a9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10971   lignes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokeniser les données"
      ],
      "metadata": {
        "id": "epgSCddlRueh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher une description\n",
        "example_data = text_data[30]\n",
        "example_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1-MQSzBkRv3a",
        "outputId": "a411c927-defc-47d3-a06b-63e1c0b12f61"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'et que je souffre comme père'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir en un Dataset TensorFlow\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ],
      "metadata": {
        "id": "scBxtEgzR0_C"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer une couche de vectorisation\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",                   # Standardiser le texte en le mettant en minuscules\n",
        "    max_tokens=VOCAB_SIZE,                 # Nombre maximal de tokens uniques dans le vocabulaire\n",
        "    output_mode=\"int\",                     # Mode de sortie où chaque token est représenté par un entier unique\n",
        "    output_sequence_length=MAX_LEN + 1,    # Longueur de sortie pour chaque séquence vectorisée\n",
        ")"
      ],
      "metadata": {
        "id": "kPi7MZnTR3n-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapter la couche aux données d'entraînement\n",
        "vectorize_layer.adapt(text_ds)\n",
        "\n",
        "# Récupérer le vocabulaire généré par la couche de vectorisation\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBHSiQZ0R6Fn",
        "outputId": "a03f687a-0f09-488e-c6e4-3b77b9aac6e5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " np.str_('et'),\n",
              " np.str_('l'),\n",
              " np.str_('de'),\n",
              " np.str_('la'),\n",
              " np.str_('le'),\n",
              " np.str_('les'),\n",
              " np.str_('des'),\n",
              " np.str_('que'),\n",
              " np.str_('dans'),\n",
              " np.str_('qui'),\n",
              " np.str_('à'),\n",
              " np.str_('est'),\n",
              " np.str_('en'),\n",
              " np.str_('un'),\n",
              " np.str_('d'),\n",
              " np.str_('je'),\n",
              " np.str_('du'),\n",
              " np.str_('qu'),\n",
              " np.str_('nous'),\n",
              " np.str_('au'),\n",
              " np.str_('ce'),\n",
              " np.str_('sur'),\n",
              " np.str_('il'),\n",
              " np.str_('vous'),\n",
              " np.str_('j'),\n",
              " np.str_('s'),\n",
              " np.str_('où'),\n",
              " np.str_('tu'),\n",
              " np.str_('une'),\n",
              " np.str_('a'),\n",
              " np.str_('on'),\n",
              " np.str_('aux'),\n",
              " np.str_('comme'),\n",
              " np.str_('tout'),\n",
              " np.str_('se'),\n",
              " np.str_('ne'),\n",
              " np.str_('pas'),\n",
              " np.str_('n'),\n",
              " np.str_('pour'),\n",
              " np.str_('mon'),\n",
              " np.str_('ces'),\n",
              " np.str_('ombre'),\n",
              " np.str_('plus'),\n",
              " np.str_('c'),\n",
              " np.str_('sans'),\n",
              " np.str_('son'),\n",
              " np.str_('dieu'),\n",
              " np.str_('sont'),\n",
              " np.str_('homme'),\n",
              " np.str_('ai'),\n",
              " np.str_('quand'),\n",
              " np.str_('nuit'),\n",
              " np.str_('me'),\n",
              " np.str_('ils'),\n",
              " np.str_('elle'),\n",
              " np.str_('sa'),\n",
              " np.str_('tous'),\n",
              " np.str_('ma'),\n",
              " np.str_('par'),\n",
              " np.str_('être'),\n",
              " np.str_('avec'),\n",
              " np.str_('sous'),\n",
              " np.str_('mes'),\n",
              " np.str_('m'),\n",
              " np.str_('âme'),\n",
              " np.str_('ses'),\n",
              " np.str_('moi'),\n",
              " np.str_('fait'),\n",
              " np.str_('cette'),\n",
              " np.str_('nos'),\n",
              " np.str_('dit'),\n",
              " np.str_('ô'),\n",
              " np.str_('leur'),\n",
              " np.str_('ciel'),\n",
              " np.str_('yeux'),\n",
              " np.str_('suis'),\n",
              " np.str_('si'),\n",
              " np.str_('ou'),\n",
              " np.str_('bien'),\n",
              " np.str_('là'),\n",
              " np.str_('leurs'),\n",
              " np.str_('t'),\n",
              " np.str_('lui'),\n",
              " np.str_('o'),\n",
              " np.str_('toi'),\n",
              " np.str_('donc'),\n",
              " np.str_('jour'),\n",
              " np.str_('même'),\n",
              " np.str_('amour'),\n",
              " np.str_('sombre'),\n",
              " np.str_('rien'),\n",
              " np.str_('ont'),\n",
              " np.str_('mort'),\n",
              " np.str_('ton'),\n",
              " np.str_('mais'),\n",
              " np.str_('vers'),\n",
              " np.str_('était'),\n",
              " np.str_('y'),\n",
              " np.str_('vie'),\n",
              " np.str_('vent'),\n",
              " np.str_('notre'),\n",
              " np.str_('dont'),\n",
              " np.str_('noir'),\n",
              " np.str_('ta'),\n",
              " np.str_('terre'),\n",
              " np.str_('deux'),\n",
              " np.str_('vos'),\n",
              " np.str_('coeur'),\n",
              " np.str_('autre'),\n",
              " np.str_('tes'),\n",
              " np.str_('te'),\n",
              " np.str_('mal'),\n",
              " np.str_('grand'),\n",
              " np.str_('fond'),\n",
              " np.str_('bas'),\n",
              " np.str_('»'),\n",
              " np.str_('toutes'),\n",
              " np.str_('toute'),\n",
              " np.str_('cieux'),\n",
              " np.str_('esprit'),\n",
              " np.str_('voir'),\n",
              " np.str_('puis'),\n",
              " np.str_('tombe'),\n",
              " np.str_('va'),\n",
              " np.str_('oeil'),\n",
              " np.str_('morts'),\n",
              " np.str_('fleurs'),\n",
              " np.str_('mains'),\n",
              " np.str_('toujours'),\n",
              " np.str_('voix'),\n",
              " np.str_('peut'),\n",
              " np.str_('mère'),\n",
              " np.str_('voit'),\n",
              " np.str_('sort'),\n",
              " np.str_('soir'),\n",
              " np.str_('seul'),\n",
              " np.str_('hélas'),\n",
              " np.str_('doux'),\n",
              " np.str_('ceux'),\n",
              " np.str_('soleil'),\n",
              " np.str_('haut'),\n",
              " np.str_('enfant'),\n",
              " np.str_('devant'),\n",
              " np.str_('heure'),\n",
              " np.str_('car'),\n",
              " np.str_('pendant'),\n",
              " np.str_('ange'),\n",
              " np.str_('monde'),\n",
              " np.str_('temps'),\n",
              " np.str_('front'),\n",
              " np.str_('oh'),\n",
              " np.str_('jamais'),\n",
              " np.str_('hommes'),\n",
              " np.str_('gouffre'),\n",
              " np.str_('aube'),\n",
              " np.str_('votre'),\n",
              " np.str_('vont'),\n",
              " np.str_('lumière'),\n",
              " np.str_('bois'),\n",
              " np.str_('abîme'),\n",
              " np.str_('triste'),\n",
              " np.str_('cet'),\n",
              " np.str_('ni'),\n",
              " np.str_('font'),\n",
              " np.str_('es'),\n",
              " np.str_('entre'),\n",
              " np.str_('dis'),\n",
              " np.str_('bruit'),\n",
              " np.str_('azur'),\n",
              " np.str_('astres'),\n",
              " np.str_('ainsi'),\n",
              " np.str_('travers'),\n",
              " np.str_('enfants'),\n",
              " np.str_('encore'),\n",
              " np.str_('vu'),\n",
              " np.str_('près'),\n",
              " np.str_('parmi'),\n",
              " np.str_('avoir'),\n",
              " np.str_('vieux'),\n",
              " np.str_('or'),\n",
              " np.str_('flots'),\n",
              " np.str_('deuil'),\n",
              " np.str_('aurore'),\n",
              " np.str_('vient'),\n",
              " np.str_('noirs'),\n",
              " np.str_('loin'),\n",
              " np.str_('grands'),\n",
              " np.str_('beau'),\n",
              " np.str_('immense'),\n",
              " np.str_('viens'),\n",
              " np.str_('quel'),\n",
              " np.str_('peu'),\n",
              " np.str_('paris'),\n",
              " np.str_('livre'),\n",
              " np.str_('ayant'),\n",
              " np.str_('sommes'),\n",
              " np.str_('porte'),\n",
              " np.str_('oiseaux'),\n",
              " np.str_('oiseau'),\n",
              " np.str_('main'),\n",
              " np.str_('infini'),\n",
              " np.str_('étoiles'),\n",
              " np.str_('âmes'),\n",
              " np.str_('tête'),\n",
              " np.str_('ténèbres'),\n",
              " np.str_('joie'),\n",
              " np.str_('herbe'),\n",
              " np.str_('feu'),\n",
              " np.str_('eau'),\n",
              " np.str_('avez'),\n",
              " np.str_('tant'),\n",
              " np.str_('rêve'),\n",
              " np.str_('père'),\n",
              " np.str_('puisque'),\n",
              " np.str_('mer'),\n",
              " np.str_('jours'),\n",
              " np.str_('faut'),\n",
              " np.str_('douleur'),\n",
              " np.str_('pierre'),\n",
              " np.str_('non'),\n",
              " np.str_('nature'),\n",
              " np.str_('clarté'),\n",
              " np.str_('bouche'),\n",
              " np.str_('voyant'),\n",
              " np.str_('souffle'),\n",
              " np.str_('rire'),\n",
              " np.str_('regarde'),\n",
              " np.str_('plein'),\n",
              " np.str_('onde'),\n",
              " np.str_('mystère'),\n",
              " np.str_('monstre'),\n",
              " np.str_('fois'),\n",
              " np.str_('eux'),\n",
              " np.str_('avait'),\n",
              " np.str_('air'),\n",
              " np.str_('vois'),\n",
              " np.str_('vivants'),\n",
              " np.str_('sombres'),\n",
              " np.str_('femme'),\n",
              " np.str_('chaque'),\n",
              " np.str_('étais'),\n",
              " np.str_('tombeau'),\n",
              " np.str_('regard'),\n",
              " np.str_('pleurs'),\n",
              " np.str_('matin'),\n",
              " np.str_('horreur'),\n",
              " np.str_('horizon'),\n",
              " np.str_('étoile'),\n",
              " np.str_('voilà'),\n",
              " np.str_('sens'),\n",
              " np.str_('passe'),\n",
              " np.str_('parfois'),\n",
              " np.str_('oui'),\n",
              " np.str_('maintenant'),\n",
              " np.str_('aussi'),\n",
              " np.str_('as'),\n",
              " np.str_('alors'),\n",
              " np.str_('aime'),\n",
              " np.str_('aile'),\n",
              " np.str_('vents'),\n",
              " np.str_('sais'),\n",
              " np.str_('quelque'),\n",
              " np.str_('pourquoi'),\n",
              " np.str_('femmes'),\n",
              " np.str_('faire'),\n",
              " np.str_('coeurs'),\n",
              " np.str_('bras'),\n",
              " np.str_('arbre'),\n",
              " np.str_('êtres'),\n",
              " np.str_('êtes'),\n",
              " np.str_('sang'),\n",
              " np.str_('sait'),\n",
              " np.str_('rayon'),\n",
              " np.str_('pieds'),\n",
              " np.str_('parce'),\n",
              " np.str_('océan'),\n",
              " np.str_('obscur'),\n",
              " np.str_('fin'),\n",
              " np.str_('bon'),\n",
              " np.str_('avant'),\n",
              " np.str_('éternité'),\n",
              " np.str_('univers'),\n",
              " np.str_('spectre'),\n",
              " np.str_('rit'),\n",
              " np.str_('quoi'),\n",
              " np.str_('peine'),\n",
              " np.str_('jusqu'),\n",
              " np.str_('fleur'),\n",
              " np.str_('création'),\n",
              " np.str_('après'),\n",
              " np.str_('aigle'),\n",
              " np.str_('ah'),\n",
              " np.str_('1855'),\n",
              " np.str_('·'),\n",
              " np.str_('voici'),\n",
              " np.str_('vis'),\n",
              " np.str_('ville'),\n",
              " np.str_('veut'),\n",
              " np.str_('tour'),\n",
              " np.str_('pâle'),\n",
              " np.str_('lueur'),\n",
              " np.str_('gloire'),\n",
              " np.str_('fils'),\n",
              " np.str_('fais'),\n",
              " np.str_('choses'),\n",
              " np.str_('veux'),\n",
              " np.str_('silence'),\n",
              " np.str_('rayons'),\n",
              " np.str_('nombre'),\n",
              " np.str_('mêle'),\n",
              " np.str_('morne'),\n",
              " np.str_('marine'),\n",
              " np.str_('corps'),\n",
              " np.str_('cheveux'),\n",
              " np.str_('astre'),\n",
              " np.str_('été'),\n",
              " np.str_('étaient'),\n",
              " np.str_('vit'),\n",
              " np.str_('terrace'),\n",
              " np.str_('soit'),\n",
              " np.str_('pauvre'),\n",
              " np.str_('passé'),\n",
              " np.str_('mourir'),\n",
              " np.str_('maison'),\n",
              " np.str_('flot'),\n",
              " np.str_('fille'),\n",
              " np.str_('dire'),\n",
              " np.str_('destin'),\n",
              " np.str_('bleu'),\n",
              " np.str_('anges'),\n",
              " np.str_('sinistre'),\n",
              " np.str_('profond'),\n",
              " np.str_('parle'),\n",
              " np.str_('ouvre'),\n",
              " np.str_('moment'),\n",
              " np.str_('milieu'),\n",
              " np.str_('marche'),\n",
              " np.str_('fut'),\n",
              " np.str_('doute'),\n",
              " np.str_('disait'),\n",
              " np.str_('derrière'),\n",
              " np.str_('debout'),\n",
              " np.str_('belle'),\n",
              " np.str_('avais'),\n",
              " np.str_('autres'),\n",
              " np.str_('automne'),\n",
              " np.str_('ans'),\n",
              " np.str_('ailes'),\n",
              " np.str_('énorme'),\n",
              " np.str_('songe'),\n",
              " np.str_('roses'),\n",
              " np.str_('rose'),\n",
              " np.str_('pur'),\n",
              " np.str_('monte'),\n",
              " np.str_('ici'),\n",
              " np.str_('genoux'),\n",
              " np.str_('fuit'),\n",
              " np.str_('foule'),\n",
              " np.str_('fosse'),\n",
              " np.str_('fit'),\n",
              " np.str_('encor'),\n",
              " np.str_('devient'),\n",
              " np.str_('cris'),\n",
              " np.str_('chose'),\n",
              " np.str_('chair'),\n",
              " np.str_('cendre'),\n",
              " np.str_('bête'),\n",
              " np.str_('bord'),\n",
              " np.str_('blanc'),\n",
              " np.str_('avons'),\n",
              " np.str_('tandis'),\n",
              " np.str_('seuil'),\n",
              " np.str_('quelqu'),\n",
              " np.str_('point'),\n",
              " np.str_('mers'),\n",
              " np.str_('lune'),\n",
              " np.str_('joyeux'),\n",
              " np.str_('froid'),\n",
              " np.str_('flamme'),\n",
              " np.str_('firmament'),\n",
              " np.str_('douleurs'),\n",
              " np.str_('chacun'),\n",
              " np.str_('celui'),\n",
              " np.str_('arbres'),\n",
              " np.str_('âpre'),\n",
              " np.str_('sent'),\n",
              " np.str_('semble'),\n",
              " np.str_('roi'),\n",
              " np.str_('pris'),\n",
              " np.str_('peur'),\n",
              " np.str_('petit'),\n",
              " np.str_('nuits'),\n",
              " np.str_('moins'),\n",
              " np.str_('hui'),\n",
              " np.str_('entend'),\n",
              " np.str_('elles'),\n",
              " np.str_('effrayant'),\n",
              " np.str_('chemin'),\n",
              " np.str_('aujourd'),\n",
              " np.str_('éternelle'),\n",
              " np.str_('écoute'),\n",
              " np.str_('vivre'),\n",
              " np.str_('trop'),\n",
              " np.str_('trois'),\n",
              " np.str_('souffre'),\n",
              " np.str_('raison'),\n",
              " np.str_('quatre'),\n",
              " np.str_('profonds'),\n",
              " np.str_('pleure'),\n",
              " np.str_('pleins'),\n",
              " np.str_('passant'),\n",
              " np.str_('ombres'),\n",
              " np.str_('nom'),\n",
              " np.str_('meurt'),\n",
              " np.str_('matière'),\n",
              " np.str_('lentement'),\n",
              " np.str_('jeune'),\n",
              " np.str_('horrible'),\n",
              " np.str_('hiver'),\n",
              " np.str_('fleuve'),\n",
              " np.str_('flammes'),\n",
              " np.str_('côté'),\n",
              " np.str_('champs'),\n",
              " np.str_('calme'),\n",
              " np.str_('brume'),\n",
              " np.str_('vivant'),\n",
              " np.str_('très'),\n",
              " np.str_('souviens'),\n",
              " np.str_('songeur'),\n",
              " np.str_('septembre'),\n",
              " np.str_('saint'),\n",
              " np.str_('quelle'),\n",
              " np.str_('partout'),\n",
              " np.str_('morte'),\n",
              " np.str_('malheur'),\n",
              " np.str_('lys'),\n",
              " np.str_('lieu'),\n",
              " np.str_('laisse'),\n",
              " np.str_('grande'),\n",
              " np.str_('funèbres'),\n",
              " np.str_('fronts'),\n",
              " np.str_('feuilles'),\n",
              " np.str_('enfin'),\n",
              " np.str_('emplit'),\n",
              " np.str_('donne'),\n",
              " np.str_('crois'),\n",
              " np.str_('autour'),\n",
              " np.str_('éternel'),\n",
              " np.str_('voiles'),\n",
              " np.str_('vague'),\n",
              " np.str_('têtes'),\n",
              " np.str_('sois'),\n",
              " np.str_('soeur'),\n",
              " np.str_('seigneur'),\n",
              " np.str_('rocher'),\n",
              " np.str_('prends'),\n",
              " np.str_('plume'),\n",
              " np.str_('pierres'),\n",
              " np.str_('ouragan'),\n",
              " np.str_('obscure'),\n",
              " np.str_('luit'),\n",
              " np.str_('juste'),\n",
              " np.str_('histoire'),\n",
              " np.str_('heureux'),\n",
              " np.str_('goutte'),\n",
              " np.str_('forêt'),\n",
              " np.str_('forme'),\n",
              " np.str_('déjà'),\n",
              " np.str_('douce'),\n",
              " np.str_('depuis'),\n",
              " np.str_('comment'),\n",
              " np.str_('cimetière'),\n",
              " np.str_('beauté'),\n",
              " np.str_('avril'),\n",
              " np.str_('affreux'),\n",
              " np.str_('étant'),\n",
              " np.str_('écume'),\n",
              " np.str_('vin'),\n",
              " np.str_('vais'),\n",
              " np.str_('sourire'),\n",
              " np.str_('route'),\n",
              " np.str_('rois'),\n",
              " np.str_('pitié'),\n",
              " np.str_('petite'),\n",
              " np.str_('néant'),\n",
              " np.str_('nomme'),\n",
              " np.str_('noire'),\n",
              " np.str_('mémoire'),\n",
              " np.str_('mystérieux'),\n",
              " np.str_('mot'),\n",
              " np.str_('marquis'),\n",
              " np.str_('lueurs'),\n",
              " np.str_('loi'),\n",
              " np.str_('liberté'),\n",
              " np.str_('inconnu'),\n",
              " np.str_('immensité'),\n",
              " np.str_('humaine'),\n",
              " np.str_('humain'),\n",
              " np.str_('hors'),\n",
              " np.str_('grâce'),\n",
              " np.str_('face'),\n",
              " np.str_('dormez'),\n",
              " np.str_('descend'),\n",
              " np.str_('christ'),\n",
              " np.str_('blancs'),\n",
              " np.str_('blanche'),\n",
              " np.str_('amours'),\n",
              " np.str_('adieu'),\n",
              " np.str_('vol'),\n",
              " np.str_('voile'),\n",
              " np.str_('vingt'),\n",
              " np.str_('vaste'),\n",
              " np.str_('vagues'),\n",
              " np.str_('souvent'),\n",
              " np.str_('sortir'),\n",
              " np.str_('solitude'),\n",
              " np.str_('sainte'),\n",
              " np.str_('reste'),\n",
              " np.str_('quelques'),\n",
              " np.str_('pâles'),\n",
              " np.str_('printemps'),\n",
              " np.str_('première'),\n",
              " np.str_('pense'),\n",
              " np.str_('passer'),\n",
              " np.str_('nul'),\n",
              " np.str_('mille'),\n",
              " np.str_('lit'),\n",
              " np.str_('humble'),\n",
              " np.str_('fruits'),\n",
              " np.str_('farouche'),\n",
              " np.str_('doigts'),\n",
              " np.str_('demain'),\n",
              " np.str_('chante'),\n",
              " np.str_('cercueil'),\n",
              " np.str_('cela'),\n",
              " np.str_('brille'),\n",
              " np.str_('branches'),\n",
              " np.str_('avenir'),\n",
              " np.str_('aller'),\n",
              " np.str_('1854'),\n",
              " np.str_('vision'),\n",
              " np.str_('vertu'),\n",
              " np.str_('vain'),\n",
              " np.str_('tremble'),\n",
              " np.str_('terrible'),\n",
              " np.str_('soleils'),\n",
              " np.str_('sera'),\n",
              " np.str_('rhin'),\n",
              " np.str_('profonde'),\n",
              " np.str_('petits'),\n",
              " np.str_('pensif'),\n",
              " np.str_('passent'),\n",
              " np.str_('orgueil'),\n",
              " np.str_('mur'),\n",
              " np.str_('montagne'),\n",
              " np.str_('mondes'),\n",
              " np.str_('moments'),\n",
              " np.str_('mai'),\n",
              " np.str_('lève'),\n",
              " np.str_('longtemps'),\n",
              " np.str_('jésus'),\n",
              " np.str_('jersey'),\n",
              " np.str_('jadis'),\n",
              " np.str_('haine'),\n",
              " np.str_('garde'),\n",
              " np.str_('flambeau'),\n",
              " np.str_('faites'),\n",
              " np.str_('espoir'),\n",
              " np.str_('espace'),\n",
              " np.str_('dès'),\n",
              " np.str_('doit'),\n",
              " np.str_('doigt'),\n",
              " np.str_('dites'),\n",
              " np.str_('croit'),\n",
              " np.str_('coup'),\n",
              " np.str_('chant'),\n",
              " np.str_('assez'),\n",
              " np.str_('aimer'),\n",
              " np.str_('«je'),\n",
              " np.str_('voyons'),\n",
              " np.str_('voyez'),\n",
              " np.str_('vide'),\n",
              " np.str_('ver'),\n",
              " np.str_('venir'),\n",
              " np.str_('tient'),\n",
              " np.str_('savoir'),\n",
              " np.str_('puits'),\n",
              " np.str_('poëte'),\n",
              " np.str_('peuple'),\n",
              " np.str_('pensée'),\n",
              " np.str_('penche'),\n",
              " np.str_('parler'),\n",
              " np.str_('nus'),\n",
              " np.str_('met'),\n",
              " np.str_('lointain'),\n",
              " np.str_('jette'),\n",
              " np.str_('jardin'),\n",
              " np.str_('idéal'),\n",
              " np.str_('hideux'),\n",
              " np.str_('formidable'),\n",
              " np.str_('exil'),\n",
              " np.str_('esprits'),\n",
              " np.str_('double'),\n",
              " np.str_('disent'),\n",
              " np.str_('deuils'),\n",
              " np.str_('dessus'),\n",
              " np.str_('chien'),\n",
              " np.str_('charmant'),\n",
              " np.str_('chants'),\n",
              " np.str_('cesse'),\n",
              " np.str_('cause'),\n",
              " np.str_('blême'),\n",
              " np.str_('baisers'),\n",
              " np.str_('baiser'),\n",
              " np.str_('appelle'),\n",
              " np.str_('amis'),\n",
              " np.str_('voyais'),\n",
              " np.str_('vierge'),\n",
              " np.str_('verts'),\n",
              " np.str_('tombeaux'),\n",
              " np.str_('sépulcre'),\n",
              " np.str_('signe'),\n",
              " np.str_('seuls'),\n",
              " np.str_('rue'),\n",
              " np.str_('prière'),\n",
              " np.str_('pont'),\n",
              " np.str_('pleurer'),\n",
              " np.str_('pensées'),\n",
              " np.str_('paroles'),\n",
              " np.str_('obscurité'),\n",
              " np.str_('nuée'),\n",
              " np.str_('nu'),\n",
              " np.str_('monts'),\n",
              " np.str_('mis'),\n",
              " np.str_('linceul'),\n",
              " np.str_('libre'),\n",
              " np.str_('laissant'),\n",
              " np.str_('heures'),\n",
              " np.str_('grandes'),\n",
              " np.str_('fumée'),\n",
              " np.str_('flotte'),\n",
              " np.str_('espérance'),\n",
              " np.str_('entends'),\n",
              " np.str_('droit'),\n",
              " np.str_('dort'),\n",
              " np.str_('dieux'),\n",
              " np.str_('devoir'),\n",
              " np.str_('croire'),\n",
              " np.str_('crime'),\n",
              " np.str_('cri'),\n",
              " np.str_('contre'),\n",
              " np.str_('chez'),\n",
              " np.str_('celle'),\n",
              " np.str_('bonheur'),\n",
              " np.str_('beaux'),\n",
              " np.str_('aveugle'),\n",
              " np.str_('ait'),\n",
              " np.str_('éclore'),\n",
              " np.str_('éclair'),\n",
              " np.str_('viennent'),\n",
              " np.str_('vieillard'),\n",
              " np.str_('vas'),\n",
              " np.str_('ténébreux'),\n",
              " np.str_('tourne'),\n",
              " np.str_('tempête'),\n",
              " np.str_('sublime'),\n",
              " np.str_('souvenir'),\n",
              " np.str_('sourd'),\n",
              " np.str_('soudain'),\n",
              " np.str_('soif'),\n",
              " np.str_('semblait'),\n",
              " np.str_('savez'),\n",
              " np.str_('rome'),\n",
              " np.str_('rive'),\n",
              " np.str_('prunelle'),\n",
              " np.str_('plis'),\n",
              " np.str_('plafond'),\n",
              " np.str_('pied'),\n",
              " np.str_('pauvres'),\n",
              " np.str_('paix'),\n",
              " np.str_('os'),\n",
              " np.str_('nuées'),\n",
              " np.str_('nuages'),\n",
              " np.str_('nid'),\n",
              " np.str_('neige'),\n",
              " np.str_('mêmes'),\n",
              " np.str_('mortes'),\n",
              " np.str_('mord'),\n",
              " np.str_('monstrueux'),\n",
              " np.str_('mieux'),\n",
              " np.str_('mauvais'),\n",
              " np.str_('lèvres'),\n",
              " np.str_('lugubre'),\n",
              " np.str_('lorsque'),\n",
              " np.str_('lois'),\n",
              " np.str_('larmes'),\n",
              " np.str_('irai'),\n",
              " np.str_('invisible'),\n",
              " np.str_('instant'),\n",
              " np.str_('france'),\n",
              " np.str_('fort'),\n",
              " np.str_('feux'),\n",
              " np.str_('fantôme'),\n",
              " np.str_('fange'),\n",
              " np.str_('eût'),\n",
              " np.str_('envie'),\n",
              " np.str_('effroi'),\n",
              " np.str_('destins'),\n",
              " np.str_('dame'),\n",
              " np.str_('clartés'),\n",
              " np.str_('cherche'),\n",
              " np.str_('charmants'),\n",
              " np.str_('blanches'),\n",
              " np.str_('auguste'),\n",
              " np.str_('amère'),\n",
              " np.str_('île'),\n",
              " np.str_('église'),\n",
              " np.str_('âge'),\n",
              " np.str_('vrai'),\n",
              " np.str_('voulu'),\n",
              " np.str_('voulez'),\n",
              " np.str_('voie'),\n",
              " np.str_('vil'),\n",
              " np.str_('v'),\n",
              " np.str_('troupeau'),\n",
              " np.str_('tremblant'),\n",
              " np.str_('tombent'),\n",
              " np.str_('tendre'),\n",
              " np.str_('suit'),\n",
              " np.str_('solitaire'),\n",
              " np.str_('silencieux'),\n",
              " np.str_('seule'),\n",
              " np.str_('serein'),\n",
              " np.str_('sept'),\n",
              " np.str_('savent'),\n",
              " np.str_('robe'),\n",
              " np.str_('regards'),\n",
              " np.str_('regarder'),\n",
              " np.str_('purs'),\n",
              " np.str_('proscrit'),\n",
              " np.str_('propre'),\n",
              " np.str_('profondeurs'),\n",
              " np.str_('problème'),\n",
              " np.str_('prières'),\n",
              " np.str_('presque'),\n",
              " np.str_('prend'),\n",
              " np.str_('pousse'),\n",
              " np.str_('pourtant'),\n",
              " np.str_('plaine'),\n",
              " np.str_('paupière'),\n",
              " np.str_('passants'),\n",
              " np.str_('parfum'),\n",
              " np.str_('ouverte'),\n",
              " np.str_('nids'),\n",
              " np.str_('naît'),\n",
              " np.str_('monter'),\n",
              " np.str_('mont'),\n",
              " np.str_('maître'),\n",
              " np.str_('marie'),\n",
              " np.str_('long'),\n",
              " np.str_('jeté'),\n",
              " np.str_('iv'),\n",
              " np.str_('ii'),\n",
              " np.str_('grève'),\n",
              " np.str_('force'),\n",
              " np.str_('faute'),\n",
              " np.str_('faisait'),\n",
              " np.str_('enfer'),\n",
              " np.str_('eaux'),\n",
              " np.str_('doucement'),\n",
              " np.str_('dessous'),\n",
              " np.str_('crâne'),\n",
              " np.str_('crie'),\n",
              " np.str_('connaît'),\n",
              " np.str_('colombe'),\n",
              " np.str_('chansons'),\n",
              " np.str_('bleus'),\n",
              " np.str_('autrefois'),\n",
              " np.str_('aura'),\n",
              " np.str_('apparaître'),\n",
              " np.str_('angoisse'),\n",
              " np.str_('amant'),\n",
              " np.str_('allez'),\n",
              " np.str_('aimé'),\n",
              " np.str_('1846'),\n",
              " np.str_('énigme'),\n",
              " np.str_('éclaire'),\n",
              " np.str_('échelle'),\n",
              " np.str_('éblouissement'),\n",
              " np.str_('vérité'),\n",
              " np.str_('vénus'),\n",
              " np.str_('voûte'),\n",
              " np.str_('villes'),\n",
              " np.str_('tranquille'),\n",
              " np.str_('torture'),\n",
              " np.str_('suffit'),\n",
              " np.str_('splendeur'),\n",
              " np.str_('souffrance'),\n",
              " np.str_('souffles'),\n",
              " np.str_('sonore'),\n",
              " np.str_('sirènes'),\n",
              " np.str_('sapins'),\n",
              " np.str_('sage'),\n",
              " np.str_('rochers'),\n",
              " np.str_('rend'),\n",
              " np.str_('quelquefois'),\n",
              " np.str_('prêtres'),\n",
              " np.str_('prêtre'),\n",
              " np.str_('profondes'),\n",
              " np.str_('portes'),\n",
              " np.str_('pleine'),\n",
              " np.str_('plaît'),\n",
              " np.str_('personne'),\n",
              " np.str_('parfums'),\n",
              " np.str_('odeur'),\n",
              " np.str_('nues'),\n",
              " np.str_('nuage'),\n",
              " np.str_('murmure'),\n",
              " np.str_('mourant'),\n",
              " np.str_('maux'),\n",
              " np.str_('mars'),\n",
              " np.str_('marches'),\n",
              " np.str_('marchant'),\n",
              " np.str_('luire'),\n",
              " np.str_('louis'),\n",
              " np.str_('lorsqu'),\n",
              " np.str_('longs'),\n",
              " np.str_('livide'),\n",
              " np.str_('langue'),\n",
              " np.str_('jéhovah'),\n",
              " np.str_('juin'),\n",
              " np.str_('jean'),\n",
              " np.str_('iii'),\n",
              " np.str_('humanité'),\n",
              " np.str_('herbes'),\n",
              " np.str_('genre'),\n",
              " np.str_('frère'),\n",
              " np.str_('frissonner'),\n",
              " np.str_('frissonne'),\n",
              " np.str_('foudre'),\n",
              " np.str_('fatal'),\n",
              " np.str_('erreur'),\n",
              " np.str_('errer'),\n",
              " np.str_('entendre'),\n",
              " np.str_('entendons'),\n",
              " np.str_('ensemble'),\n",
              " np.str_('effet'),\n",
              " np.str_('désert'),\n",
              " np.str_('drame'),\n",
              " np.str_('disant'),\n",
              " np.str_('dents'),\n",
              " np.str_('croix'),\n",
              " np.str_('content'),\n",
              " np.str_('constellations'),\n",
              " np.str_('connais'),\n",
              " np.str_('coin'),\n",
              " np.str_('claire'),\n",
              " np.str_('clair'),\n",
              " np.str_('chute'),\n",
              " np.str_('chaîne'),\n",
              " np.str_('chanter'),\n",
              " np.str_('chantent'),\n",
              " np.str_('cachot'),\n",
              " np.str_('béni'),\n",
              " np.str_('bords'),\n",
              " np.str_('bons'),\n",
              " np.str_('boire'),\n",
              " np.str_('beaucoup'),\n",
              " np.str_('attends'),\n",
              " np.str_('arrive'),\n",
              " np.str_('argent'),\n",
              " np.str_('apparaît'),\n",
              " np.str_('allait'),\n",
              " np.str_('allais'),\n",
              " np.str_('abîmes'),\n",
              " np.str_('abord'),\n",
              " np.str_('épreuve'),\n",
              " np.str_('élève'),\n",
              " np.str_('écueil'),\n",
              " np.str_('écrit'),\n",
              " np.str_('écoutez'),\n",
              " np.str_('éblouissant'),\n",
              " np.str_('vieilles'),\n",
              " np.str_('vi'),\n",
              " np.str_('verra'),\n",
              " np.str_('vermeil'),\n",
              " np.str_('venait'),\n",
              " np.str_('tâche'),\n",
              " np.str_('tremblants'),\n",
              " np.str_('traîne'),\n",
              " np.str_('tournant'),\n",
              " np.str_('tomber'),\n",
              " np.str_('table'),\n",
              " np.str_('sévère'),\n",
              " np.str_('su'),\n",
              " np.str_('source'),\n",
              " np.str_('souffrons'),\n",
              " np.str_('songes'),\n",
              " np.str_('serait'),\n",
              " np.str_('seine'),\n",
              " np.str_('sages'),\n",
              " np.str_('sache'),\n",
              " np.str_('rêveur'),\n",
              " np.str_('réclame'),\n",
              " np.str_('rampe'),\n",
              " np.str_('radieux'),\n",
              " np.str_('puisqu'),\n",
              " np.str_('présent'),\n",
              " np.str_('profondeur'),\n",
              " np.str_('prison'),\n",
              " np.str_('poésie'),\n",
              " np.str_('pouvoir'),\n",
              " np.str_('plonge'),\n",
              " np.str_('pleurent'),\n",
              " np.str_('perdu'),\n",
              " np.str_('penseurs'),\n",
              " np.str_('paupières'),\n",
              " np.str_('parts'),\n",
              " np.str_('parole'),\n",
              " np.str_('parlent'),\n",
              " np.str_('pareils'),\n",
              " np.str_('page'),\n",
              " np.str_('ouvrir'),\n",
              " np.str_('ouvrant'),\n",
              " np.str_('ouvert'),\n",
              " np.str_('nue'),\n",
              " np.str_('neuf'),\n",
              " np.str_('naître'),\n",
              " np.str_('murs'),\n",
              " np.str_('misère'),\n",
              " np.str_('miel'),\n",
              " np.str_('matelots'),\n",
              " np.str_('malheureux'),\n",
              " np.str_('malgré'),\n",
              " np.str_('maisons'),\n",
              " np.str_('loup'),\n",
              " np.str_('las'),\n",
              " np.str_('laisser'),\n",
              " np.str_('juillet'),\n",
              " np.str_('immobile'),\n",
              " np.str_('ignore'),\n",
              " np.str_('i'),\n",
              " np.str_('hymne'),\n",
              " np.str_('hier'),\n",
              " np.str_('gronde'),\n",
              " np.str_('gouffres'),\n",
              " np.str_('globe'),\n",
              " np.str_('frémissant'),\n",
              " np.str_('froids'),\n",
              " np.str_('froide'),\n",
              " np.str_('frisson'),\n",
              " np.str_('fou'),\n",
              " np.str_('forêts'),\n",
              " np.str_('foi'),\n",
              " np.str_('flamboie'),\n",
              " np.str_('fixe'),\n",
              " np.str_('ferme'),\n",
              " np.str_('fenêtre'),\n",
              " np.str_('faux'),\n",
              " np.str_('fauve'),\n",
              " np.str_('famille'),\n",
              " np.str_('faite'),\n",
              " np.str_('faim'),\n",
              " np.str_('faces'),\n",
              " np.str_('extase'),\n",
              " np.str_('entrer'),\n",
              " np.str_('entendais'),\n",
              " np.str_('désespoir'),\n",
              " np.str_('démons'),\n",
              " np.str_('degrés'),\n",
              " np.str_('dedans'),\n",
              " np.str_('danse'),\n",
              " np.str_('coule'),\n",
              " np.str_('colombes'),\n",
              " np.str_('cimes'),\n",
              " np.str_('chantant'),\n",
              " np.str_('chanson'),\n",
              " np.str_('champ'),\n",
              " np.str_('caresse'),\n",
              " np.str_('bêtes'),\n",
              " np.str_('brise'),\n",
              " np.str_('brin'),\n",
              " np.str_('blancheur'),\n",
              " np.str_('belles'),\n",
              " np.str_('autant'),\n",
              " np.str_('aurait'),\n",
              " np.str_('aucun'),\n",
              " np.str_('arrière'),\n",
              " np.str_('antre'),\n",
              " np.str_('années'),\n",
              " np.str_('allons'),\n",
              " np.str_('aimée'),\n",
              " np.str_('affreuse'),\n",
              " np.str_('adam'),\n",
              " np.str_('4'),\n",
              " np.str_('étrange'),\n",
              " np.str_('ébloui'),\n",
              " np.str_('vue'),\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher quelques correspondances entre tokens et mots\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6WS26TfR_UK",
        "outputId": "1756597f-5e49-44df-b782-5dee61d41c42"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: et\n",
            "3: l\n",
            "4: de\n",
            "5: la\n",
            "6: le\n",
            "7: les\n",
            "8: des\n",
            "9: que\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVJE0mv3SDUc",
        "outputId": "ad2d5b86-62e4-44d2-e525-bb7341d43a12"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8961"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher le même exemple converti en entiers\n",
        "example_tokenised = vectorize_layer(example_data)\n",
        "\n",
        "# Afficher la séquence d'entiers résultante\n",
        "print(example_tokenised.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJxeabwKSGJ5",
        "outputId": "65ddc548-d4a8-4d5b-97c6-7327fa3dc08d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  2   9  17 406  34 214   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Créer l'ensemble d'entraînement"
      ],
      "metadata": {
        "id": "3kORVntnSJEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_inputs(text):\n",
        "    \"\"\"\n",
        "    Prépare les entrées pour l'entraînement en créant des paires de phrases :\n",
        "    l'une avec le texte original et l'autre avec le texte décalé d'un mot.\n",
        "\n",
        "    Args:\n",
        "        text (tf.Tensor): Tensor contenant le texte brut.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Deux tensors, l'un avec le texte original (x) et l'autre avec le texte décalé d'un mot (y).\n",
        "    \"\"\"\n",
        "\n",
        "    # Étendre les dimensions du texte pour le traitement\n",
        "    text = tf.expand_dims(text, -1)\n",
        "\n",
        "    # Convertir le texte en séquences d'entiers\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "\n",
        "    # x contient tous les mots sauf le dernier de chaque phrase\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "\n",
        "    # y contient tous les mots sauf le premier de chaque phrase, décalant ainsi les séquences d'un mot\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "\n",
        "    return x, y\n",
        "\n",
        "# Appliquer la fonction `prepare_inputs` à l'ensemble de données\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ],
      "metadata": {
        "id": "MaQ12KD8SOvb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_output = train_ds.take(1).get_single_element()"
      ],
      "metadata": {
        "id": "Xnaf2QVYSQBn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# exemple\n",
        "example_input_output[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M7B4m8MSUrW",
        "outputId": "3bf8d862-2e78-428a-9127-fa68702cf642"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
              "array([  30,   53,    9,   26,  345,  144,   64,   76, 1444,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0])>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple de sortie (décalé d'un token)\n",
        "example_input_output[1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b3IrNswSXbI",
        "outputId": "f3b870bc-55d6-4db9-afe2-a63000bdc7ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
              "array([  53,    9,   26,  345,  144,   64,   76, 1444,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0])>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Créer la fonction de masquage pour l'attention causale"
      ],
      "metadata": {
        "id": "ZGhuAc_RSkxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Crée un masque pour l'attention causale.\n",
        "\n",
        "    Ce masque permet de s'assurer que lors de la prédiction d'un token,\n",
        "    seul le passé (les tokens précédents) est pris en compte.\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): Taille du lot.\n",
        "        n_dest (int): Nombre de tokens de destination.\n",
        "        n_src (int): Nombre de tokens source.\n",
        "        dtype (tf.DType): Type des éléments du masque.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Masque d'attention causale de forme [batch_size, n_dest, n_src].\n",
        "    \"\"\"\n",
        "\n",
        "    # Crée des indices pour les tokens de destination et source\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "\n",
        "    # Détermine la relation entre les indices de destination et de source\n",
        "    m = i >= j - n_src + n_dest\n",
        "\n",
        "    # Convertit le masque booléen en type spécifié\n",
        "    mask = tf.cast(m, dtype)\n",
        "\n",
        "    # Redimensionne le masque pour le rendre compatible avec les dimensions attendues\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "\n",
        "    # Duplique le masque pour tout le lot\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "# Affiche le masque d'attention causale transposé pour une meilleure visualisation\n",
        "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcQtGcd-SqsE",
        "outputId": "f3689082-062e-42d5-ca5c-d930ec97cee9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Créer une couche de bloc Transformer"
      ],
      "metadata": {
        "id": "Lv1HzvHPSt3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    \"\"\"\n",
        "    Couche de bloc Transformer.\n",
        "\n",
        "    Ce bloc est composé d'une attention multi-têtes suivie d'une\n",
        "    normalisation de couche et d'un réseau feed-forward.\n",
        "\n",
        "    Attributes:\n",
        "        num_heads (int): Nombre de têtes pour l'attention multi-têtes.\n",
        "        key_dim (int): Dimension de la clé pour l'attention.\n",
        "        embed_dim (int): Dimension de l'embedding.\n",
        "        ff_dim (int): Dimension du réseau feed-forward interne.\n",
        "        dropout_rate (float, optional): Taux de dropout. Par défaut à 0.1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.ff_dim = ff_dim\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # Initialisation des couches\n",
        "        self.attn = layers.MultiHeadAttention(\n",
        "            num_heads, key_dim, output_shape=embed_dim\n",
        "        )\n",
        "        self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
        "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
        "        self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
        "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Passage en avant du bloc Transformer.\"\"\"\n",
        "\n",
        "        # Calcul du masque d'attention causale\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(\n",
        "            batch_size, seq_len, seq_len, tf.bool\n",
        "        )\n",
        "\n",
        "        # Application de l'attention multi-têtes\n",
        "        attention_output, attention_scores = self.attn(\n",
        "            inputs,\n",
        "            inputs,\n",
        "            attention_mask=causal_mask,\n",
        "            return_attention_scores=True,\n",
        "        )\n",
        "        attention_output = self.dropout_1(attention_output)\n",
        "        out1 = self.ln_1(inputs + attention_output)\n",
        "\n",
        "        # Application du réseau feed-forward\n",
        "        ffn_1 = self.ffn_1(out1)\n",
        "        ffn_2 = self.ffn_2(ffn_1)\n",
        "        ffn_output = self.dropout_2(ffn_2)\n",
        "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Retourne la configuration du bloc Transformer.\"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"key_dim\": self.key_dim,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "                \"num_heads\": self.num_heads,\n",
        "                \"ff_dim\": self.ff_dim,\n",
        "                \"dropout_rate\": self.dropout_rate,\n",
        "            }\n",
        "        )\n",
        "        return config"
      ],
      "metadata": {
        "id": "StQQhLvBSvJd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Créer l'Embedding de Tokens et de Position"
      ],
      "metadata": {
        "id": "X0pAkoPmTI1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    \"\"\"\n",
        "    Couche d'embedding pour les tokens et les positions.\n",
        "\n",
        "    Cette couche génère des embeddings à la fois pour les tokens (mots ou caractères)\n",
        "    et pour leurs positions respectives dans une séquence. L'ajout des embeddings de\n",
        "    position est une technique couramment utilisée dans des modèles comme Transformer.\n",
        "\n",
        "    Attributs:\n",
        "    - max_len: Longueur maximale de la séquence.\n",
        "    - vocab_size: Taille du vocabulaire.\n",
        "    - embed_dim: Dimension de l'embedding.\n",
        "    - token_emb: Couche d'embedding pour les tokens.\n",
        "    - pos_emb: Couche d'embedding pour les positions.\n",
        "\n",
        "    Méthodes:\n",
        "    - call: Génère les embeddings pour une séquence donnée.\n",
        "    - get_config: Récupère la configuration de la couche pour la sauvegarde et la restauration.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_len, vocab_size, embed_dim):\n",
        "        \"\"\"\n",
        "        Initialise la couche avec les paramètres donnés.\n",
        "\n",
        "        Paramètres:\n",
        "        - max_len (int): Longueur maximale de la séquence.\n",
        "        - vocab_size (int): Taille du vocabulaire.\n",
        "        - embed_dim (int): Dimension de l'embedding.\n",
        "        \"\"\"\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.max_len = max_len\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        # Initialisation de la couche d'embedding pour les tokens\n",
        "        self.token_emb = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        # Initialisation de la couche d'embedding pour les positions\n",
        "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        Génère les embeddings pour une séquence donnée.\n",
        "\n",
        "        Paramètres:\n",
        "        - x (Tensor): Séquence d'entrée.\n",
        "\n",
        "        Retour:\n",
        "        - Tensor: Embeddings des tokens augmentés des embeddings de position.\n",
        "        \"\"\"\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Récupère la configuration de la couche.\n",
        "\n",
        "        Cette méthode est utilisée pour la sauvegarde et la restauration de la couche.\n",
        "\n",
        "        Retour:\n",
        "        - dict: Dictionnaire contenant la configuration de la couche.\n",
        "        \"\"\"\n",
        "        config = super().get_config()\n",
        "        config.update(\n",
        "            {\n",
        "                \"max_len\": self.max_len,\n",
        "                \"vocab_size\": self.vocab_size,\n",
        "                \"embed_dim\": self.embed_dim,\n",
        "            }\n",
        "        )\n",
        "        return config\n"
      ],
      "metadata": {
        "id": "5WY6CldcTKZl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Construire le modèle Transformer"
      ],
      "metadata": {
        "id": "9fFG70egTzxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Création de l'entrée du modèle. C'est un tensor d'entiers (indices des tokens)\n",
        "# avec une longueur indéfinie (d'où le 'None').\n",
        "inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "# Application de la couche d'embedding pour les tokens et les positions.\n",
        "x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "\n",
        "# Application du bloc Transformer, qui renvoie à la fois la sortie du bloc\n",
        "# et les scores d'attention.\n",
        "x, attention_scores = TransformerBlock(\n",
        "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM\n",
        ")(x)\n",
        "\n",
        "# La sortie est une densité de probabilités sur l'ensemble du vocabulaire\n",
        "# (taille VOCAB_SIZE) pour chaque position de la séquence d'entrée.\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "\n",
        "# Création du modèle GPT en liant les entrées et les sorties définies précédemment.\n",
        "gpt = models.Model(inputs=inputs, outputs=[outputs, attention_scores])\n",
        "\n",
        "# Compilation du modèle avec l'optimiseur Adam et une fonction de perte\n",
        "# pour la classification multi-classes. La deuxième perte est définie\n",
        "# comme 'None' car nous n'entraînons pas le modèle sur les scores d'attention.\n",
        "gpt.compile(\"adam\", loss=[losses.SparseCategoricalCrossentropy(), None])"
      ],
      "metadata": {
        "id": "Dc9CT5FWT5DR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "RWMYuV1lT8H_",
        "outputId": "295c831e-fe25-4ff4-f365-14942e22ea14"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m2,580,480\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),    │       \u001b[38;5;34m658,688\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)] │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)    │     \u001b[38;5;34m2,570,000\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580,480</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ transformer_block               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">658,688</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)] │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570,000</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,809,168\u001b[0m (22.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,809,168</span> (22.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,809,168\u001b[0m (22.16 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,809,168</span> (22.16 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "    # model.load_weights('./models/model')\n",
        "    gpt = models.load_model(\"./models/gpt\", compile=True)"
      ],
      "metadata": {
        "id": "cKYhbeSXUAtq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entraîner le Transformer"
      ],
      "metadata": {
        "id": "F07yqGThUETn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextGenerator(callbacks.Callback):\n",
        "    \"\"\"\n",
        "    Callback pour générer du texte à la fin de chaque époque pendant l'entraînement d'un modèle.\n",
        "\n",
        "    Attributs:\n",
        "        index_to_word (list): Liste des mots, indexée par leurs indices.\n",
        "        word_to_index (dict): Dictionnaire des mots et de leurs indices correspondants.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        \"\"\"\n",
        "        Initialise le générateur de texte.\n",
        "\n",
        "        Args:\n",
        "            index_to_word (list): Liste des mots, indexée par leurs indices.\n",
        "            top_k (int, optional): Nombre de meilleurs mots à considérer pour le sampling. Par défaut à 10.\n",
        "        \"\"\"\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index for index, word in enumerate(index_to_word)\n",
        "        }\n",
        "\n",
        "    def sample_from(self, probs, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Échantillonne un indice de mot à partir de probabilités données.\n",
        "\n",
        "        Args:\n",
        "            probs (list): Liste des probabilités.\n",
        "            temperature (float): Paramètre pour contrôler le degré d'aléatoire lors de l'échantillonnage.\n",
        "\n",
        "        Returns:\n",
        "            int: Indice échantillonné.\n",
        "            list: Probabilités ajustées.\n",
        "        \"\"\"\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature=1.0):\n",
        "        \"\"\"\n",
        "        Génère un texte à partir d'un prompt initial.\n",
        "\n",
        "        Args:\n",
        "            start_prompt (str): Prompt initial pour la génération de texte.\n",
        "            max_tokens (int): Nombre maximal de mots à générer.\n",
        "            temperature (float): Paramètre pour contrôler le degré d'aléatoire lors de l'échantillonnage.\n",
        "\n",
        "        Returns:\n",
        "            list: Liste contenant des informations sur chaque mot généré.\n",
        "        \"\"\"\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "            x = np.array([start_tokens])\n",
        "            y, att = self.model.predict(x, verbose=0)\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "            info.append(\n",
        "                {\n",
        "                    \"prompt\": start_prompt,\n",
        "                    \"word_probs\": probs,\n",
        "                    \"atts\": att[0, :, -1, :],\n",
        "                }\n",
        "            )\n",
        "            start_tokens.append(sample_token)\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\"\n",
        "        Méthode appelée à la fin de chaque époque pendant l'entraînement.\n",
        "\n",
        "        Args:\n",
        "            epoch (int): Numéro de l'époque actuelle.\n",
        "            logs (dict, optional): Dictionnaire des logs d'entraînement.\n",
        "        \"\"\"\n",
        "        self.generate(\"la vie\", max_tokens=80, temperature=1.0)"
      ],
      "metadata": {
        "id": "Z85dvDVLUK1t"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model save checkpoint\n",
        "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
        "    filepath=\"./checkpoint/checkpoint.weights.h5\",\n",
        "    save_weights_only=True,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=0,\n",
        ")\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "# Tokenize starting prompt\n",
        "text_generator = TextGenerator(vocab)"
      ],
      "metadata": {
        "id": "_RI3FkJ8UNr9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oc_ZdLkJV_R4",
        "outputId": "63fc1b53-b127-4af1-8074-f10691fcd9a3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m342/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.2850\n",
            "generated text:\n",
            "la vie y rit \n",
            "\n",
            "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 52ms/step - loss: 1.2813\n",
            "Epoch 2/10\n",
            "\u001b[1m341/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3758\n",
            "generated text:\n",
            "la vie tomba les ténèbres l heure hélas \n",
            "\n",
            "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 33ms/step - loss: 0.3758\n",
            "Epoch 3/10\n",
            "\u001b[1m341/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.3063\n",
            "generated text:\n",
            "la vie et le facteur enfuis ou de tout le jardin \n",
            "\n",
            "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 44ms/step - loss: 0.3064\n",
            "Epoch 4/10\n",
            "\u001b[1m342/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2829\n",
            "generated text:\n",
            "la vie est sombre est la muraille \n",
            "\n",
            "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - loss: 0.2829\n",
            "Epoch 5/10\n",
            "\u001b[1m342/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.2388\n",
            "generated text:\n",
            "la vie est bientôt soûle \n",
            "\n",
            "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - loss: 0.2389\n",
            "Epoch 6/10\n",
            "\u001b[1m341/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.2072\n",
            "generated text:\n",
            "la vie de taupe au lynx \n",
            "\n",
            "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - loss: 0.2074\n",
            "Epoch 7/10\n",
            "\u001b[1m342/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1834\n",
            "generated text:\n",
            "la vie est une cour de lune \n",
            "\n",
            "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 25ms/step - loss: 0.1835\n",
            "Epoch 8/10\n",
            "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.1508\n",
            "generated text:\n",
            "la vie au songe et toutes ces strophes ensemble \n",
            "\n",
            "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - loss: 0.1509\n",
            "Epoch 9/10\n",
            "\u001b[1m342/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1235\n",
            "generated text:\n",
            "la vie \n",
            "\n",
            "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - loss: 0.1235\n",
            "Epoch 10/10\n",
            "\u001b[1m341/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.1077\n",
            "generated text:\n",
            "la vie est variable aussi bien que l euripe \n",
            "\n",
            "\u001b[1m343/343\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - loss: 0.1078\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x786a7e96b860>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer le dossier s’il n’existe pas\n",
        "import os\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "\n",
        "# Sauvegarder le modèle complet\n",
        "gpt.save(\"models/gpt.keras\")"
      ],
      "metadata": {
        "id": "ktgktUuuWXGs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    \"\"\"\n",
        "    Affiche le texte avec une mise en évidence basée sur les scores d'attention\n",
        "    et imprime les probabilités des `top_k` mots les plus probables.\n",
        "\n",
        "    Paramètres:\n",
        "    - info (list) : Liste contenant des informations sur le prompt, les scores d'attention et les probabilités des mots.\n",
        "    - vocab (list) : Liste de mots représentant le vocabulaire.\n",
        "    - top_k (int) : Nombre de mots les plus probables à afficher.\n",
        "\n",
        "    \"\"\"\n",
        "    # Pour chaque élément dans 'info' (chaque mot généré et ses détails associés)\n",
        "    for i in info:\n",
        "        highlighted_text = []\n",
        "\n",
        "        # Calculer la mise en évidence du texte en fonction des scores d'attention\n",
        "        for word, att_score in zip(\n",
        "            i[\"prompt\"].split(), np.mean(i[\"atts\"], axis=0)\n",
        "        ):\n",
        "            highlighted_text.append(\n",
        "                # Crée une mise en évidence basée sur le score d'attention pour le mot actuel\n",
        "                '<span style=\"background-color:rgba(135,206,250,'\n",
        "                + str(att_score / max(np.mean(i[\"atts\"], axis=0)))\n",
        "                + ');\">'\n",
        "                + word\n",
        "                + \"</span>\"\n",
        "            )\n",
        "        highlighted_text = \" \".join(highlighted_text)\n",
        "\n",
        "        # Affiche le texte mis en évidence\n",
        "        display(HTML(highlighted_text))\n",
        "\n",
        "        # Obtenir les probabilités du mot généré\n",
        "        word_probs = i[\"word_probs\"]\n",
        "\n",
        "        # Obtenir les indices et les probabilités des `top_k` mots les plus probables\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "\n",
        "        # Afficher chaque mot et sa probabilité associée\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "\n",
        "        # Imprimer une ligne de séparation\n",
        "        print(\"--------\\n\")"
      ],
      "metadata": {
        "id": "FMnsQ9IiWcI4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"le ciel\", max_tokens=1000, temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0mlnF41We8n",
        "outputId": "839a9f12-ee0a-4992-ee04-6dbcf674c406"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "le ciel s en va l ombre et qui pleure \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"Un matin\", max_tokens=2000, temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ShAfVyoWg5V",
        "outputId": "fbfd59b8-8f04-40bf-c072-d78c24c363fe"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "Un matin je me promène \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"la\", max_tokens=80, temperature=0.5\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "kcckjhqmWjYY",
        "outputId": "86e6e936-94df-4967-9abe-2621d166859d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "la chambre est veuve \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:rgba(135,206,250,1.0);\">la</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mouche:   \t16.579999923706055%\n",
            "ronce:   \t10.550000190734863%\n",
            "nature:   \t9.4399995803833%\n",
            "rumeur:   \t9.40999984741211%\n",
            "boulangère:   \t5.929999828338623%\n",
            "--------\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:rgba(135,206,250,1.0);\">la</span> <span style=\"background-color:rgba(135,206,250,0.98947126);\">chambre</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "est:   \t99.95999908447266%\n",
            "a:   \t0.029999999329447746%\n",
            "était:   \t0.0%\n",
            "peut:   \t0.0%\n",
            "au:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:rgba(135,206,250,0.31660873);\">la</span> <span style=\"background-color:rgba(135,206,250,1.0);\">chambre</span> <span style=\"background-color:rgba(135,206,250,0.055369582);\">est</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "veuve:   \t100.0%\n",
            "goutte:   \t0.0%\n",
            "mort:   \t0.0%\n",
            "étroit:   \t0.0%\n",
            "lumière:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background-color:rgba(135,206,250,0.6056878);\">la</span> <span style=\"background-color:rgba(135,206,250,1.0);\">chambre</span> <span style=\"background-color:rgba(135,206,250,0.7298447);\">est</span> <span style=\"background-color:rgba(135,206,250,0.89642537);\">veuve</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":   \t100.0%\n",
            "qui:   \t0.0%\n",
            "de:   \t0.0%\n",
            "divin:   \t0.0%\n",
            "ce:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}